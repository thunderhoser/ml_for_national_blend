#!/bin/bash

#SBATCH --job-name="train_neural_net"
#SBATCH --partition="fge"
#SBATCH --account="mdl-sti"
#SBATCH --qos="gpuwf"
#SBATCH --nodes=1
#SBATCH --ntasks=8           # 8 tasks per node
#SBATCH --cpus-per-task=2
#SBATCH --ntasks-per-node=8  # 8 GPUs per node
#SBATCH --exclusive
#SBATCH --time=168:00:00
#SBATCH --exclude=h28n12
##SBATCH --exclude=h31n03,h28n12
##SBATCH --exclude=h29n01,h26n05,h28n06,h29n07,h30n03,h30n15
#SBATCH --mail-type=BEGIN,END,FAIL
#SBATCH --mail-user=ryan.lagerquist@noaa.gov
#SBATCH --output=train_neural_net_%A.out

# You need to load the CUDA library before calling train_neural_net.py.  This is used to accelerate the training on GPUs.
# You also need to submit training jobs to the fge partition, which contains 800 GPUs (100 nodes with 8 GPUs each).
# Without GPUs, these models are so big that we don't stand a chance at training them in any reasonable time frame.
module load cuda/12.3.1

# Load conda environment.
conda init
conda activate base

# Print a few sanity checks to the command line.
echo `which conda`
echo `which python`
echo `which python3`
echo $PATH

# Define where the code is.
CODE_DIR_NAME="/scratch1/RDARCH/rda-ghpcs/Ryan.Lagerquist/ml_for_national_blend_standalone/ml_for_national_blend"

# Define where the NN architecture (template file) is.
TEMPLATE_DIR_NAME="/scratch1/RDARCH/rda-ghpcs/Ryan.Lagerquist/ml_for_national_blend_models/u_net_architecture/template"

# Define where you want the trained model to be saved.
OUTPUT_DIR_NAME="/scratch1/RDARCH/rda-ghpcs/Ryan.Lagerquist/ml_for_national_blend_models/u_net_architecture"

# Define where the truth data (URMA target fields) can be found.
TARGET_DIR_NAME="/scratch2/NCEPDEV/stmp/Ryan.Lagerquist/ml_for_national_blend_project/urma_data_final/processed"

# Define the dictionary mapping NWP models to predictor variables.
NWP_FIELD_NAME_DICT='{"ensemble":["pressure_mean_sea_level_pascals","pressure_surface_pascals","temperature_2m_agl_kelvins","dewpoint_2m_agl_kelvins","u_wind_10m_agl_m_s01","v_wind_10m_agl_m_s01","accumulated_precip_metres","wind_gust_10m_agl_m_s01"],"hrrr":["pressure_mean_sea_level_pascals","pressure_surface_pascals","temperature_2m_agl_kelvins","dewpoint_2m_agl_kelvins","u_wind_10m_agl_m_s01","v_wind_10m_agl_m_s01","accumulated_precip_metres"],"wrf_arw":["pressure_mean_sea_level_pascals","pressure_surface_pascals","temperature_2m_agl_kelvins","dewpoint_2m_agl_kelvins","u_wind_10m_agl_m_s01","v_wind_10m_agl_m_s01","accumulated_precip_metres"],"nam_nest":["pressure_mean_sea_level_pascals","pressure_surface_pascals","temperature_2m_agl_kelvins","dewpoint_2m_agl_kelvins","u_wind_10m_agl_m_s01","v_wind_10m_agl_m_s01","accumulated_precip_metres"],"gridded_lamp":["temperature_2m_agl_kelvins","dewpoint_2m_agl_kelvins","u_wind_10m_agl_m_s01","v_wind_10m_agl_m_s01","wind_gust_10m_agl_m_s01"],"gridded_gfs_mos":["temperature_2m_agl_kelvins","dewpoint_2m_agl_kelvins","u_wind_10m_agl_m_s01","v_wind_10m_agl_m_s01","wind_gust_10m_agl_m_s01"]}'

# Define where normalization params for NWP data can be found.
NWP_NORMALIZATION_FILE_NAME="/scratch2/NCEPDEV/stmp/Ryan.Lagerquist/ml_for_national_blend_project/nwp_model_data_final/normalization_params_long300_20170101-20211220.nc"

# Call train_u_net.py and train the model.
python3 -u "${CODE_DIR_NAME}/train_u_net.py" \
--input_template_file_name="${TEMPLATE_DIR_NAME}/model.keras" \
--output_model_dir_name="${OUTPUT_DIR_NAME}" \
--nwp_lead_time_hours 48 \
--nwp_model_names "ensemble" "gridded_gfs_mos" "gridded_lamp" "hrrr" "nam_nest" "wrf_arw" \
--nwp_model_to_field_names=${NWP_FIELD_NAME_DICT} \
--nwp_normalization_file_name="${NWP_NORMALIZATION_FILE_NAME}" \
--backup_nwp_model_name="ensemble" \
--backup_nwp_dir_name="/scratch2/STI/mdl-sti/Ryan.Lagerquist/ml_for_national_blend_project/nwp_model_data_final/ensemble_all_vars/processed/interp_to_nbm_grid" \
--target_lead_time_hours=48 \
--target_field_names "temperature_2m_agl_kelvins" "u_wind_10m_agl_m_s01" "v_wind_10m_agl_m_s01" "dewpoint_2m_agl_kelvins" "wind_gust_10m_agl_m_s01" \
--compare_to_baseline_in_loss=1 \
--num_examples_per_batch=1 \
--sentinel_value=-10 \
--patch_size_2pt5km_pixels=208 \
--patch_buffer_size_2pt5km_pixels=50 \
--patch_overlap_size_2pt5km_pixels=64 \
--resid_baseline_model_name="ensemble" \
--resid_baseline_lead_time_hours=48 \
--resid_baseline_model_dir_name="/scratch2/NCEPDEV/stmp/Ryan.Lagerquist/ml_for_national_blend_project/nwp_model_data_final/ensemble/processed/interp_to_nbm_grid" \
--first_init_time_strings_for_training "2017-01-01-00" \
--last_init_time_strings_for_training "2021-12-20-18" \
--nwp_directory_names "/scratch2/STI/mdl-sti/Ryan.Lagerquist/ml_for_national_blend_project/nwp_model_data_final/ensemble_all_vars/processed/interp_to_nbm_grid" "/scratch2/NCEPDEV/stmp/Ryan.Lagerquist/ml_for_national_blend_project/nwp_model_data_final/gridded_gfs_mos/processed/interp_to_nbm_grid" "/scratch2/NCEPDEV/stmp/Ryan.Lagerquist/ml_for_national_blend_project/nwp_model_data_final/gridded_lamp/processed/interp_to_nbm_grid" "/scratch2/NCEPDEV/stmp/Ryan.Lagerquist/ml_for_national_blend_project/nwp_model_data_final/hrrr/processed/interp_to_nbm_grid" "/scratch2/STI/mdl-sti/Ryan.Lagerquist/ml_for_national_blend_project/nwp_model_data_final/nam_nest/processed/interp_to_nbm_grid" "/scratch2/STI/mdl-sti/Allison.Layne/preprocessed_data/wrf_arw/processed/interp_to_nbm_grid" \
--target_dir_name="${TARGET_DIR_NAME}" \
--training_init_time_limit_strings "2017-01-01-00" "2021-12-20-18" \
--validation_init_time_limit_strings "2022-01-01-00" "2022-12-20-18" \
--num_epochs=1000 \
--num_training_batches_per_epoch=3000 \
--num_validation_batches_per_epoch=1500 \
--plateau_patience_epochs=10 \
--plateau_learning_rate_multiplier=0.95 \
--early_stopping_patience_epochs=10000
