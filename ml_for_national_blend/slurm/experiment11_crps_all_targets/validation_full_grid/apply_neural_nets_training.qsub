#!/bin/bash

#SBATCH --job-name="apply_neural_nets_training"
#SBATCH --partition="fge"
#SBATCH --account="mdl-sti"
#SBATCH --qos="gpuwf"
#SBATCH --nodes=1
#SBATCH --ntasks=8           # 8 tasks per node
#SBATCH --cpus-per-task=2
#SBATCH --ntasks-per-node=8  # 8 GPUs per node
#SBATCH --exclusive
#SBATCH --time=08:00:00
#SBATCH --array=0-63
#SBATCH --exclude=h28n12
##SBATCH --exclude=h31n03,h28n12
##SBATCH --exclude=h29n01,h26n05,h28n06,h29n07,h30n03,h30n15
#SBATCH --mail-type=BEGIN,END,FAIL
#SBATCH --mail-user=ryan.lagerquist@noaa.gov
#SBATCH --output=apply_neural_nets_training_%A_%a.out

module load cuda/12.3.1
conda init
conda activate base

echo `which conda`
echo `which python`
echo `which python3`

# PATH=/usr/local/cuda/bin:$PATH
echo $PATH

CODE_DIR_NAME="/scratch1/RDARCH/rda-ghpcs/Ryan.Lagerquist/ml_for_national_blend_standalone/ml_for_national_blend"
TOP_MODEL_DIR_NAME="/scratch1/RDARCH/rda-ghpcs/Ryan.Lagerquist/ml_for_national_blend_models/experiment11_crps_all_targets"
TARGET_DIR_NAME="/scratch2/NCEPDEV/stmp/Ryan.Lagerquist/ml_for_national_blend_project/urma_data_final/processed"

NWP_LEAD_TIME_COUNTS=("1" "2" "3" "4" "1" "2" "3" "4" "1" "2" "3" "4" "1" "2" "3" "4" "1" "2" "3" "4" "1" "2" "3" "4" "1" "2" "3" "4" "1" "2" "3" "4" "1" "2" "3" "4" "1" "2" "3" "4" "1" "2" "3" "4" "1" "2" "3" "4" "1" "2" "3" "4" "1" "2" "3" "4" "1" "2" "3" "4" "1" "2" "3" "4")
LAG_TIME_COUNTS=("1" "1" "1" "1" "2" "2" "2" "2" "3" "3" "3" "3" "4" "4" "4" "4" "1" "1" "1" "1" "2" "2" "2" "2" "3" "3" "3" "3" "4" "4" "4" "4" "1" "1" "1" "1" "2" "2" "2" "2" "3" "3" "3" "3" "4" "4" "4" "4" "1" "1" "1" "1" "2" "2" "2" "2" "3" "3" "3" "3" "4" "4" "4" "4")
INIT_HOURS=("00" "00" "00" "00" "00" "00" "00" "00" "00" "00" "00" "00" "00" "00" "00" "00" "06" "06" "06" "06" "06" "06" "06" "06" "06" "06" "06" "06" "06" "06" "06" "06" "12" "12" "12" "12" "12" "12" "12" "12" "12" "12" "12" "12" "12" "12" "12" "12" "18" "18" "18" "18" "18" "18" "18" "18" "18" "18" "18" "18" "18" "18" "18" "18")

num_nwp_lead_times=${NWP_LEAD_TIME_COUNTS[$SLURM_ARRAY_TASK_ID]}
num_lag_times=${LAG_TIME_COUNTS[$SLURM_ARRAY_TASK_ID]}
init_hour=${INIT_HOURS[$SLURM_ARRAY_TASK_ID]}

model_dir_name="${TOP_MODEL_DIR_NAME}/num-nwp-lead-times=${num_nwp_lead_times}_num-lag-times=${num_lag_times}"
echo $model_dir_name

INIT_DATE_STRINGS=("2021-01-01" "2021-01-06" "2021-01-11" "2021-01-16" "2021-01-21" "2021-01-26" "2021-01-31" "2021-02-05" "2021-02-10" "2021-02-15" "2021-02-20" "2021-02-25" "2021-03-02" "2021-03-07" "2021-03-12" "2021-03-17" "2021-03-22" "2021-03-27" "2021-04-01" "2021-04-06" "2021-04-11" "2021-04-16" "2021-04-21" "2021-04-26" "2021-05-01" "2021-05-06" "2021-05-11" "2021-05-16" "2021-05-21" "2021-05-26" "2021-05-31" "2021-06-05" "2021-06-10" "2021-06-15" "2021-06-20" "2021-06-25" "2021-06-30" "2021-07-05" "2021-07-10" "2021-07-15" "2021-07-20" "2021-07-25" "2021-07-30" "2021-08-04" "2021-08-09" "2021-08-14" "2021-08-19" "2021-08-24" "2021-08-29" "2021-09-03" "2021-09-08" "2021-09-13" "2021-09-18" "2021-09-23" "2021-09-28" "2021-10-03" "2021-10-08" "2021-10-13" "2021-10-18" "2021-10-23" "2021-10-28" "2021-11-02" "2021-11-07" "2021-11-12" "2021-11-17" "2021-11-22" "2021-11-27" "2021-12-02" "2021-12-07" "2021-12-12" "2021-12-17" "2021-12-22" "2021-12-27")

for this_init_date_string in "${INIT_DATE_STRINGS[@]}"; do
    python3 -u "${CODE_DIR_NAME}/apply_neural_net.py" \
    --input_model_file_name="${model_dir_name}/model.weights.h5" \
    --init_time_string="${this_init_date_string}-${init_hour}" \
    --nwp_model_names "ensemble" "hrrr" "nam_nest" \
    --input_nwp_directory_names "/scratch2/STI/mdl-sti/Ryan.Lagerquist/ml_for_national_blend_project/nwp_model_data_final/ensemble_all_vars/processed/interp_to_nbm_grid" "/scratch2/NCEPDEV/stmp/Ryan.Lagerquist/ml_for_national_blend_project/nwp_model_data_final/hrrr/processed/interp_to_nbm_grid" "/scratch2/NCEPDEV/stmp/Ryan.Lagerquist/ml_for_national_blend_project/nwp_model_data_final/nam_nest/processed/interp_to_nbm_grid" \
    --input_target_dir_name="${TARGET_DIR_NAME}" \
    --patches_to_full_grid=1 \
    --patch_overlap_size_2pt5km_pixels=144 \
    --output_dir_name="${model_dir_name}/training_full_grid"
done
